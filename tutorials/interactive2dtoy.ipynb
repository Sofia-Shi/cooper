{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Dynamics: an Interactive Tool\n",
    "\n",
    "In this notebook we present a toy constrained optimization problem in 2D.\n",
    "\n",
    "We provide an interactive widget which shows the optimization path realized when\n",
    "solving the problem using [Cooper](https://github.com/gallego-posada/cooper).\n",
    "\n",
    "> Acknowledgement: The presented visualizations and optimization problems follow closely the blogposts by Degrave and Korshunova (2021a, 2021b):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [Setup](#setup)\n",
    "* [Constrained Minimization Problem](#cmp)\n",
    "* [Widget](#widget)\n",
    "* [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup <a class=\"anchor\" id=\"setup\"></a>\n",
    "Install Cooper, with `examples` requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e git+https://github.com/gallego-posada/cooper#egg=.[examples] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import style_utils\n",
    "import torch\n",
    "import tutorial_utils\n",
    "from IPython.display import display\n",
    "from ipywidgets import HBox, Layout, VBox, interactive\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import cooper\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Minimization Problem <a class=\"anchor\" id=\"cmp\"></a>\n",
    "\n",
    "Consider the following constrained optimization problem on\n",
    " the 2D domain $(x, y) \\in [0,\\pi/2] \\times [0,\\infty]$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\underset{x, y}{\\text{min}}\\quad f(x,y) &:= \\left(1 - \\text{sin}(x) \\right) \\ \\big(1+(y - 1)^2\\big) & \\tag{1} \\\\\n",
    "s.t. \\quad  g(x,y) &:= \\left(1 - \\text{cos}(x) \\right)\\ \\big(1+(y-1)^2\\big) - \\epsilon \\leq 0 & \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "given some $\\epsilon \\geq 0 $.\n",
    "Note how both $f$ and $g$ are convex functions in the specified domain.\n",
    "As such, this constrained minimization problem is a convex problem.\n",
    "\n",
    "The following class implements this CMP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Toy2DCMP(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, is_constrained=False, epsilon=1.0, problem_type=\"Convex\"):\n",
    "        self.problem_type = problem_type\n",
    "        self.epsilon = epsilon\n",
    "        super().__init__(is_constrained)\n",
    "\n",
    "    def closure(self, params):\n",
    "        \"\"\"This function evaluates the objective function and constraint\n",
    "        defect. It updates the attributes of this CMP based on the results.\"\"\"\n",
    "\n",
    "        x, y = params[:, 0], params[:, 1]\n",
    "\n",
    "        if self.problem_type == \"Convex\":\n",
    "            f = (1 - torch.sin(x)) * (1 + (y - 1.) ** 2)\n",
    "            # In standard form (defect <= 0)\n",
    "            g = (1 - torch.cos(x)) * (1 + (y - 1.) ** 2) - self.epsilon\n",
    "        elif self.problem_type == \"Concave\":\n",
    "            f = torch.sin(x) * (1 + (y - 1.) ** 2)\n",
    "            # in standard form (defect <= 0)\n",
    "            g = torch.cos(x) * (1 + (y - 1.) ** 2) - self.epsilon\n",
    "        else:\n",
    "            raise ValueError(\"Unknown problem type.\")\n",
    "\n",
    "        # Store the values in a CMPState as attributes\n",
    "        state = cooper.CMPState(loss=f, ineq_defect=g)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: concave $f(x, y)$ and $g(x, y)$.\n",
    "\n",
    "Associated with `Toy2DCMP(problem_type=\"Concave\")`\n",
    "\n",
    "Consider a similar optimization problem to Eq. (1), also on $[0,\\pi/2] \\times [0,\\infty]$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\underset{x, y}{\\text{min}}\\quad f(x,y) &:= \\text{sin}(x) \\ \\big(1+(y - 1)^2\\big) & \\tag{2} \\\\\n",
    "s.t. \\quad  g(x,y) &:= \\text{cos}(x)\\ \\big(1+(y-1)^2\\big) - \\epsilon \\leq 0 & \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "given some $\\epsilon \\geq 0 $.\n",
    "$f$ and $g$ are concave functions with respect to $x$ in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget <a class=\"anchor\" id=\"widget\"></a>\n",
    "\n",
    "The following hidden cell implements a widget which shows the optimization dynamics\n",
    "of Cooper when solving the problems in Eq. (1) and Bonus: Eq. (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The widget displays:\n",
    "- Loss throughout training.\n",
    "- Values of the Lagrange multiplier and constraint defect throughout training.\n",
    "- Optimization path in $(x, y)$ space. The feasible set is highlighted on blue.\n",
    "Contours for the loss function are drawn.\n",
    "- Optimization path in $(f, g)$ space. The Pareto front formed between $f$ and $g$\n",
    "is added as reference.\n",
    "\n",
    "Control items:\n",
    "- Problem type: Convex in Eq. (1) or Concave in Eq. (2)\n",
    "- Number of iterations to train for.\n",
    "- Initial values for $(x, y)$.\n",
    "- Primal optimizer class (e.g. SGD) and its learning rate.\n",
    "- Dual optimizer class and its learning rate.\n",
    "- Whether to employ dual restarts.\n",
    "- Whether to use ExtraGradient updates on the parameters and multiplier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e77538f1d864a7ea7097d3608e0347b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Problem type', options=('Convex', 'Concave'), value='Conveâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Toy2DWidget():\n",
    "    def __init__(self):\n",
    "\n",
    "        # --------------------------------------- Create some control elements\n",
    "        problem_type_dropdown = widgets.Dropdown(\n",
    "            options=[\"Convex\", \"Concave\"],\n",
    "            description=\"Problem type\",\n",
    "        )\n",
    "        epsilon_slider = widgets.FloatSlider(\n",
    "            min=-0.1, max=1.1, step=0.05, value=0.7, description=\"Const. level\"\n",
    "        )\n",
    "        primal_lr_slider = widgets.FloatLogSlider(\n",
    "            base=10,\n",
    "            min=-4,\n",
    "            max=0,\n",
    "            step=0.1,\n",
    "            value=2e-2,\n",
    "            description=\"Primal LR\",\n",
    "            continuous_update=False,\n",
    "        )\n",
    "        primal_optim_dropdown = widgets.Dropdown(\n",
    "            value=\"SGD\",\n",
    "            options=[\"SGD\", \"SGDM_0.9\", \"Adam\"],\n",
    "            description=\"Primal opt.\",\n",
    "        )\n",
    "        dual_lr_slider = widgets.FloatLogSlider(\n",
    "            base=10,\n",
    "            min=-4,\n",
    "            max=0,\n",
    "            step=0.1,\n",
    "            value=5e-1,\n",
    "            description=\"Dual LR\",\n",
    "            continuous_update=False,\n",
    "        )\n",
    "        dual_optim_dropdown = widgets.Dropdown(\n",
    "            value=\"SGD\",\n",
    "            options=[\"SGD\", \"SGDM_0.9\", \"Adam\"],\n",
    "            description=\"Dual opt.\",\n",
    "        )\n",
    "        x_slider = widgets.FloatSlider(\n",
    "            min=0,\n",
    "            max=np.pi / 2,\n",
    "            step=0.01,\n",
    "            value=0.9,\n",
    "            description=\"x init.\",\n",
    "            continuous_update=False,\n",
    "        )\n",
    "        y_slider = widgets.FloatSlider(\n",
    "            min=0,\n",
    "            max=3.0,\n",
    "            step=0.01,\n",
    "            value=2.,\n",
    "            description=\"y init.\",\n",
    "            continuous_update=False,\n",
    "        )\n",
    "        iters_textbox = widgets.BoundedIntText(\n",
    "            min=1, max=10000, value=200, description=\"Max Iters\"\n",
    "        )\n",
    "        restarts_checkbox = widgets.Checkbox(value=False, description=\"Dual restarts\")\n",
    "        extrapolation_checkbox = widgets.Checkbox(\n",
    "            value=False, description=\"Extrapolation\"\n",
    "        )\n",
    "\n",
    "        # --------------------------------- Indicate what each option observes\n",
    "        widget = interactive(\n",
    "            self.update,\n",
    "            x=x_slider,\n",
    "            y=y_slider,\n",
    "            num_iters=iters_textbox,\n",
    "            epsilon=epsilon_slider,\n",
    "            problem_type=problem_type_dropdown,\n",
    "            primal_lr=primal_lr_slider,\n",
    "            primal_optim=primal_optim_dropdown,\n",
    "            dual_lr=dual_lr_slider,\n",
    "            dual_optim=dual_optim_dropdown,\n",
    "            dual_restarts=restarts_checkbox,\n",
    "            extrapolation=extrapolation_checkbox,\n",
    "        )\n",
    "        controls_layout = Layout(\n",
    "            display=\"flex\",\n",
    "            flex_flow=\"row wrap\",\n",
    "            border=\"solid 2px\",\n",
    "            align_items=\"center\",\n",
    "            width=\"950px\",\n",
    "        )\n",
    "        controls = HBox(widget.children[:-1], layout=controls_layout)\n",
    "        output = widget.children[-1]\n",
    "        display(VBox([controls, output]))\n",
    "\n",
    "        # ------------------------------ Initialize the CMP and its formulation\n",
    "        self.cmp = Toy2DCMP(is_constrained=True)\n",
    "        self.formulation = cooper.LagrangianFormulation(self.cmp)\n",
    "\n",
    "        # # Run the update a first time\n",
    "        widget.update()\n",
    "\n",
    "    def reset_problem(self, epsilon=None, problem_type=\"convex\"):\n",
    "        \"\"\"Reset the cmp and formulation for new training loops.\"\"\"\n",
    "\n",
    "        self.cmp.problem_type = problem_type\n",
    "\n",
    "        # Reset the state of the CMP. Update epsilon if necessary.\n",
    "        self.cmp.epsilon = epsilon\n",
    "        self.cmp.state = None\n",
    "\n",
    "        # Reset multipliers\n",
    "        self.formulation.ineq_multipliers = None\n",
    "        self.formulation.eq_multipliers = None\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        problem_type,\n",
    "        epsilon,\n",
    "        num_iters,\n",
    "        primal_optim,\n",
    "        dual_optim,\n",
    "        x,\n",
    "        primal_lr,\n",
    "        dual_lr,\n",
    "        y,\n",
    "        extrapolation,\n",
    "        dual_restarts,\n",
    "    ):\n",
    "\n",
    "        # Initialize the figure\n",
    "        self.fig = plt.figure(figsize=(15, 5))\n",
    "        grid_specs = GridSpec(2, 3, figure=self.fig)\n",
    "        self.loss_iter_axis = self.fig.add_subplot(grid_specs[0, 0])\n",
    "        self.defect_iter_axis = self.fig.add_subplot(grid_specs[1, 0])\n",
    "        self.xy_axis = self.fig.add_subplot(grid_specs[:, 1])\n",
    "        self.loss_defect_axis = self.fig.add_subplot(grid_specs[:, 2])\n",
    "\n",
    "        # Reset the state of cmp and formulation. Indicate the new epsilon.\n",
    "        self.reset_problem(epsilon=epsilon, problem_type=problem_type)\n",
    "\n",
    "        # Plot the loss contours. Done once as loss does not change with sliders.\n",
    "        # The feasible set does change and is plotted in self.update.\n",
    "        self.contour_params = self.loss_contours()\n",
    "\n",
    "        # Plot the Pareto front.\n",
    "        foo = self.plot_pareto_front()\n",
    "\n",
    "        # Update the filled contour indicating the feasible set (x, y) space and\n",
    "        # epsilon hline (f, g) space\n",
    "        foo = self.plot_feasible_set()\n",
    "\n",
    "        # New initialization\n",
    "        params = torch.nn.Parameter(torch.tensor([[x, y]]))\n",
    "\n",
    "        # Construct a new optimizer\n",
    "        self.constrained_optimizer = self.create_optimizer(\n",
    "            params=params,\n",
    "            primal_optim=primal_optim,\n",
    "            dual_optim=dual_optim,\n",
    "            primal_lr=primal_lr,\n",
    "            dual_lr=dual_lr,\n",
    "            dual_restarts=dual_restarts,\n",
    "            extrapolation=extrapolation,\n",
    "        )\n",
    "\n",
    "        state_history = self.train(params=params, num_iters=num_iters)\n",
    "        self.update_trajectory_plots(state_history)\n",
    "\n",
    "    def create_optimizer(\n",
    "        self,\n",
    "        params,\n",
    "        primal_optim,\n",
    "        primal_lr,\n",
    "        dual_optim,\n",
    "        dual_lr,\n",
    "        dual_restarts,\n",
    "        extrapolation,\n",
    "    ):\n",
    "\n",
    "        # Check if any optimizer has momentum and add to kwargs it if necessary\n",
    "        primal_kwargs = {\"lr\": primal_lr}\n",
    "        if primal_optim == \"SGDM_0.9\":\n",
    "            primal_optim = \"SGD\"\n",
    "            primal_kwargs[\"momentum\"] = 0.9\n",
    "        dual_kwargs = {\"lr\": dual_lr}\n",
    "        if dual_optim == \"SGDM_0.9\":\n",
    "            dual_optim = \"SGD\"\n",
    "            dual_kwargs[\"momentum\"] = 0.9\n",
    "\n",
    "        # Indicate if we are using extrapolation\n",
    "        if extrapolation:\n",
    "            primal_optim = \"Extra\" + primal_optim\n",
    "            dual_optim = \"Extra\" + dual_optim\n",
    "\n",
    "        primal_optimizer = getattr(cooper.optim, primal_optim)(\n",
    "            [params],\n",
    "            **primal_kwargs,\n",
    "        )\n",
    "        dual_optimizer = cooper.optim.partial(\n",
    "            getattr(cooper.optim, dual_optim),\n",
    "            **dual_kwargs,\n",
    "        )\n",
    "\n",
    "        constrained_optimizer = cooper.ConstrainedOptimizer(\n",
    "            formulation=self.formulation,\n",
    "            primal_optimizer=primal_optimizer,\n",
    "            dual_optimizer=dual_optimizer,\n",
    "            dual_restarts=dual_restarts,\n",
    "        )\n",
    "\n",
    "        return constrained_optimizer\n",
    "\n",
    "    def train(self, params, num_iters):\n",
    "        \"\"\"Train.\"\"\"\n",
    "\n",
    "        # Store CMPStates and parameter values throughout the optimization process\n",
    "        state_history = tutorial_utils.StateLogger(\n",
    "            save_metrics=[\"loss\", \"ineq_defect\", \"ineq_multipliers\"]\n",
    "        )\n",
    "\n",
    "        for iter_num in range(num_iters):\n",
    "\n",
    "            self.constrained_optimizer.zero_grad()\n",
    "            lagrangian = self.formulation.composite_objective(self.cmp.closure, params)\n",
    "            self.formulation.custom_backward(lagrangian)\n",
    "            self.constrained_optimizer.step(self.cmp.closure, params)\n",
    "\n",
    "            # Ensure parameters remain in the domain of the functions\n",
    "            params[:, 0].data.clamp_(min=0, max=np.pi / 2)\n",
    "            params[:, 1].data.clamp_(min=0)\n",
    "\n",
    "            # Store optimization metrics at each step\n",
    "            state_history.store_metrics(\n",
    "                self.formulation,\n",
    "                iter_num,\n",
    "                partial_dict={\"params\": copy.deepcopy(params.data)},\n",
    "            )\n",
    "\n",
    "        return state_history\n",
    "\n",
    "    def loss_contours(self):\n",
    "        \"\"\"Plot the loss contours.\"\"\"\n",
    "        # Initial contours for plot\n",
    "        x_range = torch.tensor(np.linspace(0, np.pi / 2, 100))\n",
    "        y_range = torch.tensor(np.linspace(0, 2.0, 100))\n",
    "        grid_x, grid_y = torch.meshgrid(x_range, y_range, indexing=\"ij\")\n",
    "\n",
    "        grid_params = torch.stack([grid_x.flatten(), grid_y.flatten()], axis=1)\n",
    "        all_states = self.cmp.closure(grid_params)\n",
    "        loss_grid = all_states.loss.reshape(len(x_range), len(y_range))\n",
    "\n",
    "        # Plot the contours\n",
    "        CS = self.xy_axis.contour(\n",
    "            grid_x,\n",
    "            grid_y,\n",
    "            loss_grid,\n",
    "            levels=[0.05, 0.125, 0.25, 0.5, 1, 1.5],\n",
    "            alpha=1.0,\n",
    "            colors=\"gray\"\n",
    "        )\n",
    "\n",
    "        # Add styling\n",
    "        self.xy_axis.clabel(CS, inline=1)\n",
    "\n",
    "        defect_grid = all_states.ineq_defect.reshape(len(x_range), len(y_range))\n",
    "        return (grid_x, grid_y, defect_grid)\n",
    "\n",
    "    def plot_pareto_front(self):\n",
    "        \"\"\"Plot the Pareto front in the loss vs defect plane. This part is done\n",
    "        once.\"\"\"\n",
    "        # y parametrizes distance to front. Regardless of epsilon, y=0 poses a\n",
    "        # non-dominated solution. x parametrizes the position on the Pareto front.\n",
    "        x_range = torch.tensor(np.linspace(0, np.pi / 2, 100))\n",
    "        y_range = torch.tensor(100 * [1.])\n",
    "        all_states = self.cmp.closure(torch.stack([x_range, y_range], axis=1))\n",
    "        self.pareto_front = (all_states.loss, all_states.ineq_defect.squeeze())\n",
    "        self.loss_defect_axis.plot(\n",
    "            self.pareto_front[0], self.pareto_front[1], c=\"black\", alpha=0.7\n",
    "        )\n",
    "\n",
    "        # Add styling\n",
    "        self.loss_defect_axis.set_xlabel(r\"Objective $f$\")\n",
    "        self.loss_defect_axis.set_ylabel(r\"Constraint $g$\")\n",
    "\n",
    "    def update_trajectory_plots(self, state_history):\n",
    "\n",
    "        blue = style_utils.COLOR_DICT[\"blue\"]\n",
    "        red = style_utils.COLOR_DICT[\"red\"]\n",
    "        green = style_utils.COLOR_DICT[\"green\"]\n",
    "        yellow = style_utils.COLOR_DICT[\"yellow\"]\n",
    "\n",
    "        all_metrics = state_history.unpack_stored_metrics()\n",
    "        cmap_vals = np.linspace(0, 1, len(all_metrics[\"loss\"]))\n",
    "        cmap_name = \"viridis\"\n",
    "\n",
    "        # --------------------------------- Trajectory in x-y plane\n",
    "        params_hist = np.stack(all_metrics[\"params\"]).squeeze().reshape(-1, 2)\n",
    "\n",
    "        SC = self.xy_axis.scatter(\n",
    "            params_hist[:, 0], params_hist[:, 1], c=cmap_vals, cmap=cmap_name, s=20, alpha=0.5, zorder=10\n",
    "        )\n",
    "        # Add marker signaling the final iterate\n",
    "        self.xy_axis.scatter(\n",
    "            *params_hist[-1, :],\n",
    "            marker=\"*\",\n",
    "            s=150,\n",
    "            zorder=100,\n",
    "            c=yellow,\n",
    "        )\n",
    "        self.xy_axis.set_xlabel(r\"Param. $x$\")\n",
    "        self.xy_axis.set_ylabel(r\"Param. $y$\")\n",
    "        self.xy_axis.set_title(r\"Parameter $(x, y)$ space\")\n",
    "        # Constrain domain\n",
    "        self.xy_axis.set_xlim(0, np.pi / 2)\n",
    "        self.xy_axis.set_ylim(0, 2.0)\n",
    "\n",
    "        # -------------------------------- Trajectory in loss-defect plane\n",
    "        defects = np.stack(all_metrics[\"ineq_defect\"]).squeeze()\n",
    "        self.loss_defect_axis.scatter(all_metrics[\"loss\"], defects, alpha=0.5, s=20, c=cmap_vals, cmap=cmap_name)\n",
    "        # Add marker signaling the final iterate\n",
    "        self.loss_defect_axis.scatter(\n",
    "            all_metrics[\"loss\"][-1], defects[-1], marker=\"*\", s=150, zorder=10, c=yellow\n",
    "        )\n",
    "        self.loss_defect_axis.set_title(r\"Loss vs. constraint $(f, g)$ space\")\n",
    "        self.loss_defect_axis.set_xlim(- 0.1, 1.2)\n",
    "        self.loss_defect_axis.set_ylim(-self.cmp.epsilon - 0.1, 1.2 - self.cmp.epsilon)\n",
    "        self.loss_defect_axis.set_aspect('equal')\n",
    "\n",
    "        # -------------------------------- Loss history\n",
    "        self.loss_iter_axis.plot(\n",
    "            all_metrics[\"iters\"], all_metrics[\"loss\"], c=blue, linewidth=2\n",
    "        )\n",
    "        self.loss_iter_axis.set_title(\"Loss\")\n",
    "        self.loss_iter_axis.set_xlabel(\"Iteration\")\n",
    "\n",
    "        # -------------------------------- Multiplier and defect history\n",
    "        self.defect_iter_axis.plot(\n",
    "            all_metrics[\"iters\"],\n",
    "            defects,\n",
    "            c=red,\n",
    "            linewidth=2,\n",
    "            label=\"Defect\",\n",
    "            zorder=10,\n",
    "        )\n",
    "        self.defect_iter_axis.plot(\n",
    "            all_metrics[\"iters\"],\n",
    "            np.stack(all_metrics[\"ineq_multipliers\"]).squeeze(),\n",
    "            c=green,\n",
    "            linewidth=2,\n",
    "            label=\"Multiplier\",\n",
    "        )\n",
    "        self.defect_iter_axis.set_xlabel(\"Iteration\")\n",
    "        self.defect_iter_axis.legend(\n",
    "            ncol=2, loc=\"upper right\", bbox_to_anchor=(0.8, 1.25)\n",
    "        )\n",
    "\n",
    "        # -------------------------------- Colorbar\n",
    "        last = len(all_metrics[\"loss\"])\n",
    "        cbar = self.fig.colorbar(SC, label=\"Iteration\", ax=self.loss_defect_axis, ticks=np.linspace(0., 1., 6))\n",
    "        cbar.ax.set_yticklabels(np.arange(0, last + 1, last // 5))\n",
    "\n",
    "        self.fig.tight_layout()\n",
    "        # TODO: warning https://stackoverflow.com/questions/69999315/using-astropy-with-matplotlib-i-get-a-warning-to-call-gridfalse-first-due-to\n",
    "        # self.fig.subplots_adjust(right=1.1)\n",
    "\n",
    "\n",
    "    def plot_feasible_set(self):\n",
    "        \"\"\"Plot the feasible set.\"\"\"\n",
    "        # the values of g(x, y) have been computed in self.loss_contours for\n",
    "        # the whole grid. The feasibility boundary changes based on the epsilon\n",
    "        self.xy_axis.contourf(\n",
    "            *self.contour_params,\n",
    "            levels=[-10, 0],\n",
    "            colors=style_utils.COLOR_DICT[\"blue\"],\n",
    "            alpha=0.1,\n",
    "        )\n",
    "\n",
    "        self.defect_iter_axis.axhline(0, c=\"gray\", alpha=0.7, linestyle=\"--\")\n",
    "\n",
    "        # In loss vs defect plane, a line is drawn at the epsilon value\n",
    "        y = torch.cat((torch.tensor([- self.cmp.epsilon]), self.pareto_front[1]))\n",
    "        self.loss_defect_axis.fill_between(\n",
    "            x=torch.cat((torch.tensor([1.2]), self.pareto_front[0])),\n",
    "            y1=y,\n",
    "            y2=0,\n",
    "            where=y <= 0,\n",
    "            step=\"mid\",\n",
    "            color=style_utils.COLOR_DICT[\"blue\"],\n",
    "            alpha=0.1,\n",
    "        )\n",
    "        self.loss_defect_axis.axhline(0, c=\"gray\", alpha=0.7, linestyle=\"--\")\n",
    "\n",
    "widget = Toy2DWidget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a class=\"anchor\" id=\"references\"></a>\n",
    "\n",
    "- Degrave, J.   and Korshunova, I. Why machine learning algorithms are hard to tune and how to fix it. Engraved,   [blog](www.engraved.blog/why-machine-learning-algorithms-are-hard-to-tune/), 2021.\n",
    "- Degrave, J.   and  Korshunova, I. How we can make machine learning algorithms tunable. Engraved,   [blog](https://www.engraved.blog/how-we-can-make-machine-learning-algorithms-tunable/), 2021."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "interpreter": {
   "hash": "e95f21d6d24b9c057dc89e3e94c7b1d285f49ff39a3b96b3de58d480bdf45a1c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
