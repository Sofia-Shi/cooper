{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import cooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convex2dCMP(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, is_constrained=False, epsilon=1.0):\n",
    "        self.epsilon = epsilon\n",
    "        super().__init__(is_constrained)\n",
    "\n",
    "    def closure(self, params):\n",
    "        \"\"\"This function evaluates the objective function and constraint\n",
    "        defect. It updates the attributes of this CMP based on the results.\"\"\"\n",
    "\n",
    "        x = params[:, 0]\n",
    "        y = params[:, 1]\n",
    "\n",
    "        f = (1 - torch.sin(x)) * (1 + y**2)\n",
    "        g = (1 - torch.cos(x)) * (1 + y**2)\n",
    "        _ineq_defect = g - self.epsilon  # in standard form (defect <= 0)\n",
    "\n",
    "        # Store the values in a CMPState as attributes\n",
    "        state = cooper.CMPState(loss=f, ineq_defect=_ineq_defect, misc={\"g\": g})\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIM_DICT = {\n",
    "    \"SGD\": cooper.optim.SGD,\n",
    "    \"Adam\": cooper.optim.Adam,\n",
    "    \"Adagrad\": cooper.optim.Adagrad,\n",
    "    \"RMSprop\": cooper.optim.RMSprop,\n",
    "}\n",
    "\n",
    "# TODO: matplotlib style template\n",
    "# TODO: add a button to reset the parameters\n",
    "\n",
    "\n",
    "class Toy2DWidget(widgets.HBox):\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def update(\n",
    "        self, x, y, epsilon, primal_optim, primal_lr, dual_optim, dual_lr, dual_restarts\n",
    "    ):\n",
    "\n",
    "        # ---------------------- Following code could be run only once\n",
    "        # JC TODO: tried to have this section in init, but the widget would not\n",
    "        # update at all. Doing so would make updates faster.\n",
    "\n",
    "        # Initialize the figure\n",
    "        self.fig, (self.xy_axis, self.loss_defect_axis) = plt.subplots(\n",
    "            1, 2, figsize=(15, 6)\n",
    "        )\n",
    "\n",
    "        # Initialize the CMP and its formulation\n",
    "        self.cmp = Convex2dCMP(is_constrained=True)\n",
    "        self.formulation = cooper.LagrangianFormulation(self.cmp)\n",
    "\n",
    "        # Plot the loss contours. Done once as loss does not change with sliders.\n",
    "        # The feasible set does change and is plotted in self.update.\n",
    "        self.contour_params = self.loss_contours()\n",
    "\n",
    "        # Plot the pareto front.\n",
    "        self.pareto_front()\n",
    "\n",
    "        # ---------------------- Following code is run on every update\n",
    "        # Reset the state of cmp and formulation. Indicate the new epsilon.\n",
    "        self.reset_problem(epsilon=epsilon)\n",
    "\n",
    "        # Update the filled contour indicating the feasible set (x, y) space and\n",
    "        # epsilon hline (f, g) space\n",
    "        self.update_feasible_set(epsilon=epsilon)\n",
    "\n",
    "        params = torch.nn.Parameter(torch.tensor([[x, y]]))\n",
    "\n",
    "        # Construct a new optimizer\n",
    "        self.constrained_optimizer = self.create_optimizer(\n",
    "            params=params,\n",
    "            primal_optim=primal_optim,\n",
    "            dual_optim=dual_optim,\n",
    "            primal_kwargs={\"lr\": primal_lr},\n",
    "            dual_lr=dual_lr,\n",
    "            dual_restarts=dual_restarts,\n",
    "        )\n",
    "\n",
    "        state_history = self.train(params=params)\n",
    "        self.update_trayectory(state_history, epsilon)\n",
    "\n",
    "    def reset_problem(self, epsilon=None):\n",
    "        \"\"\"Reset the cmp and formulation for new training loops.\"\"\"\n",
    "\n",
    "        # Reset the state of the CMP. Update epsilon if necessary.\n",
    "        self.cmp.epsilon = epsilon\n",
    "        self.cmp.state = None\n",
    "\n",
    "        # Reset multipliers\n",
    "        self.formulation.ineq_multipliers = None\n",
    "        self.formulation.eq_multipliers = None\n",
    "\n",
    "    def create_optimizer(\n",
    "        self,\n",
    "        params,\n",
    "        primal_optim,\n",
    "        dual_optim,\n",
    "        primal_kwargs,\n",
    "        dual_lr,\n",
    "        dual_restarts,\n",
    "    ):\n",
    "\n",
    "        primal_optimizer = OPTIM_DICT[primal_optim](\n",
    "            [params],\n",
    "            **primal_kwargs,\n",
    "        )\n",
    "        dual_optimizer = cooper.optim.partial(\n",
    "            OPTIM_DICT[dual_optim],\n",
    "            lr=dual_lr,\n",
    "        )\n",
    "\n",
    "        constrained_optimizer = cooper.ConstrainedOptimizer(\n",
    "            formulation=self.formulation,\n",
    "            primal_optimizer=primal_optimizer,\n",
    "            dual_optimizer=dual_optimizer,\n",
    "            dual_restarts=dual_restarts,\n",
    "        )\n",
    "\n",
    "        return constrained_optimizer\n",
    "\n",
    "    def train(self, params):\n",
    "        \"\"\"Train.\"\"\"\n",
    "\n",
    "        # Store CMPStates and parameter values throughout the optimization process\n",
    "        state_history = OrderedDict()\n",
    "\n",
    "        for iter in range(2000):\n",
    "            self.constrained_optimizer.zero_grad()\n",
    "            lagrangian = self.formulation.composite_objective(self.cmp.closure, params)\n",
    "            self.formulation.custom_backward(lagrangian)\n",
    "            self.constrained_optimizer.step()\n",
    "            params[:, 0].data.clamp_(min=0, max=np.pi / 2)\n",
    "            params[:, 1].data.clamp_(min=0)\n",
    "\n",
    "            state_history[iter] = {\n",
    "                \"loss\": self.cmp.state.loss.item(),\n",
    "                \"defect\": self.cmp.state.ineq_defect.item(),\n",
    "                \"dual\": deepcopy(self.formulation.state()[0].data),\n",
    "                \"params\": deepcopy(params.data),\n",
    "            }\n",
    "\n",
    "        return state_history\n",
    "\n",
    "    def loss_contours(self):\n",
    "        \"\"\"Plot the loss contours.\"\"\"\n",
    "        # Initial contours for plot\n",
    "        x_range = torch.tensor(np.linspace(0, np.pi / 2, 100))\n",
    "        y_range = torch.tensor(np.linspace(0, 2.0, 100))\n",
    "        grid_x, grid_y = torch.meshgrid(x_range, y_range)\n",
    "\n",
    "        grid_params = torch.stack([grid_x.flatten(), grid_y.flatten()], axis=1)\n",
    "        all_states = self.cmp.closure(grid_params)\n",
    "        loss_grid = all_states.loss.reshape(len(x_range), len(y_range))\n",
    "\n",
    "        # Plot the contours\n",
    "        CS = self.xy_axis.contour(\n",
    "            grid_x,\n",
    "            grid_y,\n",
    "            loss_grid,\n",
    "            levels=[0.05, 0.125, 0.25, 0.5, 1, 1.5, 2.0, 3.0],\n",
    "            alpha=0.6,\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "\n",
    "        # Add styling\n",
    "        self.xy_axis.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "        g_grid = all_states.misc[\"g\"].reshape(len(x_range), len(y_range))\n",
    "        return {\n",
    "            \"grid_x\": grid_x,\n",
    "            \"grid_y\": grid_y,\n",
    "            \"grid_params\": grid_params,\n",
    "            \"g_grid\": g_grid,\n",
    "        }\n",
    "\n",
    "    def pareto_front(self):\n",
    "        \"\"\"Plot the pareto front in the loss vs defect plane. This part is done\n",
    "        once.\"\"\"\n",
    "        # y parametrizes distance to front. Regardless of epsilon, y=0 poses a\n",
    "        # non-dominated solution. x parametrizes the position on the pareto front.\n",
    "        x_range = torch.tensor(np.linspace(0, np.pi / 2, 100))\n",
    "        y_range = torch.tensor(100 * [0.0])\n",
    "        all_states = self.cmp.closure(torch.stack([x_range, y_range], axis=1))\n",
    "        self.loss_defect_axis.plot(\n",
    "            all_states.loss, all_states.misc[\"g\"].squeeze(), c=\"gray\", alpha=0.5\n",
    "        )\n",
    "\n",
    "        # Add styling\n",
    "        self.loss_defect_axis.set_xlabel(\"Objective\")\n",
    "        self.loss_defect_axis.set_ylabel(\"Constraint\")\n",
    "\n",
    "    def update_trayectory(self, state_history, epsilon):\n",
    "\n",
    "        # Unpack the state history\n",
    "        iters, loss_hist, defect_hist, params_hist, mult_hist = zip(\n",
    "            *[\n",
    "                (iter_num, _[\"loss\"], _[\"defect\"], _[\"params\"], _[\"dual\"])\n",
    "                for (iter_num, _) in state_history.items()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Multiplier, loss and defect history\n",
    "        # TODO: add plot\n",
    "\n",
    "        # Trajectory in x-y plane\n",
    "        params_hist = np.stack(params_hist).squeeze()\n",
    "        self.xy_axis.scatter(params_hist[:, 0], params_hist[:, 1], c=\"r\", s=10)\n",
    "        self.xy_axis.scatter(params_hist[-1, 0], params_hist[-1, 1], marker=\"*\", s=100)\n",
    "\n",
    "        # Trajectory in loss-defect plane\n",
    "        g = epsilon + np.stack(defect_hist).squeeze()\n",
    "        self.loss_defect_axis.scatter(loss_hist, g, s=2, c=\"red\")\n",
    "        self.loss_defect_axis.scatter(loss_hist[-1], g[-1], marker=\"*\", s=100)\n",
    "\n",
    "    def update_feasible_set(self, epsilon):\n",
    "        \"\"\"Plot the feasible set.\"\"\"\n",
    "        # the values of g(x, y) have been computed in self.loss_contours for\n",
    "        # the whole grid. The feasibility boundary changes based on the epsilon\n",
    "        self.xy_axis.contourf(\n",
    "            self.contour_params[\"grid_x\"],\n",
    "            self.contour_params[\"grid_y\"],\n",
    "            self.contour_params[\"g_grid\"],\n",
    "            levels=[-10, epsilon],\n",
    "            colors=\"blue\",\n",
    "            alpha=0.1,\n",
    "        )\n",
    "\n",
    "        # In loss vs defect plane, a line is drawn at the epsilon value\n",
    "        self.loss_defect_axis.axhline(epsilon, c=\"gray\", alpha=0.2, linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df96629d5a02457a87d68ce8f74153f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='initial x', max=1.5707963267948966), FloatSlider(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Toy2DWidget()\n",
    "\n",
    "# Create some control elements\n",
    "epsilon_slider = widgets.FloatSlider(\n",
    "    min=-0.1, max=1.1, step=0.1, value=0.5, description=\"Epsilon\"\n",
    ")\n",
    "primal_lr_slider = widgets.FloatLogSlider(\n",
    "    base=10, min=-4, max=-1, step=0.5, value=-2, description=\"Primal Learning Rate\"\n",
    ")\n",
    "primal_optim_dropdown = widgets.Dropdown(\n",
    "    value=\"SGD\", options=[\"SGD\", \"Adam\"], description=\"Primal Optimizer\"\n",
    ")\n",
    "dual_lr_slider = widgets.FloatLogSlider(\n",
    "    base=10, min=-4, max=-1, step=0.5, value=-2, description=\"Dual Learning Rate\"\n",
    ")\n",
    "dual_optim_dropdown = widgets.Dropdown(\n",
    "    value=\"SGD\", options=[\"SGD\", \"Adam\"], description=\"Dual Optimizer\"\n",
    ")\n",
    "x_slider = widgets.FloatSlider(\n",
    "    min=0, max=np.pi / 2, step=0.1, value=1.0, description=\"initial x\"\n",
    ")\n",
    "y_slider = widgets.FloatSlider(\n",
    "    min=0, max=2.0, step=0.1, value=0.5, description=\"initial y\"\n",
    ")\n",
    "restarts_checkbox = widgets.Checkbox(value=False, description=\"Dual restarts\")\n",
    "\n",
    "# Indicate what each option observes\n",
    "interact(\n",
    "    w.update,\n",
    "    epsilon=epsilon_slider,\n",
    "    primal_lr=primal_lr_slider,\n",
    "    primal_optim=primal_optim_dropdown,\n",
    "    dual_lr=dual_lr_slider,\n",
    "    dual_optim=dual_optim_dropdown,\n",
    "    dual_restarts=restarts_checkbox,\n",
    "    x=x_slider,\n",
    "    y=y_slider,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e95f21d6d24b9c057dc89e3e94c7b1d285f49ff39a3b96b3de58d480bdf45a1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('coop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
