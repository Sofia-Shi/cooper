{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Tutorial name\n",
    "This is a tutorial showcasing the functionality of cooper in the context of a convex constrained optimization problem.\n",
    "\n",
    "It is inspired by Engraved Blogpost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Install cooper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/gallego-posada/cooper\n",
    "# TODO: .[examples], try from scratch\n",
    "# !python setup.py install torch_cooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cooper\n",
    "from copy import deepcopy as copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained Minimization Problem\n",
    "\n",
    "Cooper supports constrained minimization problems with the following form:\n",
    "\\begin{align}\n",
    "    & \\underset{x}{\\text{min}}\\, & f(x) \\tag{1} \\\\\n",
    "    & \\text{s.t.} & g(x) \\leq 0 \\\\\n",
    "    & & h(x) = 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we consider the following constrained optimization problem on\n",
    " the 2D domain $(x, y) \\in [0,\\pi/2] \\times [0,\\infty]$\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{x, y}{\\text{min}}\\quad f(x,y) &:= \\left(1 - \\text{sin}(x) \\right) \\ \\big(1+y^2\\big) & \\tag{2} \\\\\n",
    "\n",
    "s.t. \\quad  g(x,y) &:= \\left(1 - \\text{cos}(x) \\right)\\ \\big(1+y^2\\big) \\leq \\epsilon & \\\\\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "given some $\\epsilon \\geq 0 $.\n",
    "Note how both $f$ and $g$ are convex functions in the specified domain.\n",
    "As such, this constrained minimization problem is a convex problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Constrained Minimization Problem. This object holds a state,\n",
    "# containing the current values of the loss (f), inequality constraint defects (g),\n",
    "# and equality constraint defects (h).\n",
    "\n",
    "\n",
    "class Convex2dCMP(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, is_constrained=False, epsilon=1.0):\n",
    "        self.epsilon = epsilon\n",
    "        super().__init__(is_constrained)\n",
    "\n",
    "    def closure(self, params):\n",
    "        \"\"\"This function evaluates the objective function and constraint\n",
    "        defect. It updates the attributes of this CMP based on the results.\"\"\"\n",
    "\n",
    "        x = params[0]\n",
    "        y = params[1]\n",
    "\n",
    "        assert x >= 0 and x <= np.pi / 2\n",
    "        assert y >= 0\n",
    "\n",
    "        f = (1 - torch.sin(x)) * (1 + y**2)\n",
    "        g = (1 - torch.cos(x)) * (1 + y**2)\n",
    "\n",
    "        # Define the constraint defects\n",
    "        _ineq_defect = g - self.epsilon  # in standard form (defect <= 0)\n",
    "        _eq_defect = None  # no equality constraints\n",
    "\n",
    "        # Store the values in a CMPState as attributes\n",
    "        state = cooper.CMPState(\n",
    "            loss=f,\n",
    "            eq_defect=_eq_defect,\n",
    "            ineq_defect=_ineq_defect,\n",
    "            # other information can be stored in the misc dictionary. cooper\n",
    "            # will not access misc and does not require it.\n",
    "            misc={'g': g.data},\n",
    "            )\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ConvexCMP.\n",
    "EPSILON = 0.7\n",
    "cmp = Convex2dCMP(is_constrained=True, epsilon=EPSILON)\n",
    "\n",
    "# cmp.state is currently None, as we have not populated it yet.\n",
    "print(cmp.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to solve problems formulated as Eq. (1). In particular,\n",
    "given our choice of problem in Eq. (2), we have convergence guarantees if we\n",
    "employ this or that to solve it. can\n",
    "\n",
    "Let us check the solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "\n",
    "Cooper has been designed to be used on neural network training in Pytorch.\n",
    "As such, we use the Lagrangian formulation of Eq. (2) and optimize its respective\n",
    "min-max game:\n",
    "\n",
    "\\begin{align*}\n",
    "& \\underset{x, y}{\\text{min}}\\ \\underset{\\lambda \\geq 0}{\\text{max}}\\ f(x,y) +\n",
    "\\lambda \\big( g(x,y) - \\epsilon \\big) \\tag{3} \\\\\n",
    "& \\underset{x, y}{\\text{min}}\\ \\underset{\\lambda \\geq 0}{\\text{max}}\\,\n",
    "\\big(1 - \\text{sin}(x) \\big) \\ \\big(1+y^2\\big) +\n",
    "\\lambda \\bigg( \\big(1 - \\text{cos}(x) \\big)\\ \\big(1+y^2\\big) - \\epsilon \\bigg) \\\\\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "Now we setup a Lagrangian formulation for ConstrainedMinimizationProblem `cmp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, multipliers are initialized to zero. Otherwise, we could provide\n",
    "# ineq_init or eq_init for\n",
    "formulation = cooper.LagrangianFormulation(cmp, ineq_init=None, eq_init=None)\n",
    "\n",
    "# Lambda is held inside the formulation object.\n",
    "multipliers = formulation.dual_parameters\n",
    "print(multipliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Now, construct the optimizer\n",
    "Lagrangian via gradient descent ascent schemes.\n",
    "Compatible with any torch optimizer on the primal and dual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.nn.Parameter(torch.tensor([1.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper around torch optimizers\n",
    "primal_optimizer = cooper.optim.SGD([params], lr=1e-2, momentum=0.0)\n",
    "# Dual variables are initialized internally by the formulation when the shape\n",
    "# of defects is identified. We provide a partial initializariion of the dual optim\n",
    "dual_optimizer = cooper.optim.partial(cooper.optim.SGD, lr=1e-4)\n",
    "\n",
    "# Initialize the constrained optimizer, which handles internally updates of the\n",
    "# primal and dual variables.\n",
    "constrained_optimizer = cooper.ConstrainedOptimizer(\n",
    "    formulation=formulation,\n",
    "    primal_optimizer=primal_optimizer,\n",
    "    dual_optimizer=dual_optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "The training loop is very similar to a vanilla pytorch loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store CMPStates and parameter values throughout the optimization process\n",
    "params_history = [copy(params.data)]\n",
    "multiplier_history = [0.]\n",
    "loss_history = []\n",
    "g_history = []\n",
    "\n",
    "iters = range(0, 1001)\n",
    "for iter in iters:\n",
    "    # Zero the gradients wrt the parameters and multipliers\n",
    "    constrained_optimizer.zero_grad()\n",
    "\n",
    "    # Calculate the lagrangian.\n",
    "    lagrangian = formulation.composite_objective(\n",
    "        cmp.closure, # forward-like function, computing loss and defects\n",
    "        params=params, # args and kwargs required for closure\n",
    "        )\n",
    "\n",
    "    # cmp.state has been updated inplace by formulation.composite_objective\n",
    "    loss_history.append(cmp.state.loss.item())\n",
    "    g_history.append(cmp.state.misc['g'].item())\n",
    "\n",
    "    # Backward\n",
    "    formulation.custom_backward(lagrangian)\n",
    "\n",
    "    # Step\n",
    "    constrained_optimizer.step()\n",
    "\n",
    "    # Clip parameters\n",
    "    min = torch.tensor([0.0, 0.0])\n",
    "    max = torch.tensor([np.pi / 2, np.inf])\n",
    "    params.data.clamp_(min=min, max=max)\n",
    "\n",
    "    # Store new parameter values\n",
    "    params_history.append(copy(params.data))\n",
    "    multiplier = formulation.dual_parameters[0]\n",
    "    multiplier_history.append(copy(multiplier.item()))\n",
    "\n",
    "    if iter % 200 == 0:\n",
    "        # Print progress\n",
    "        print(\"\\n Iteration: {}\".format(iter))\n",
    "        print(f\"  Loss: {cmp.state.loss.item(): 0.3e}\")\n",
    "        print(f\"  Defect: {copy(cmp.state.ineq_defect.item()): 0.3e}\")\n",
    "\n",
    "# CMPState after the final iteration\n",
    "final_state = cmp.closure(params)\n",
    "\n",
    "loss_history.append(final_state.loss.item())\n",
    "g_history.append(final_state.misc['g'].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        ],\n",
       "       [1.0108061 , 0.9968294 ],\n",
       "       [1.0213957 , 0.99378407],\n",
       "       ...,\n",
       "       [1.5012965 , 0.40976575],\n",
       "       [1.501255  , 0.40918794],\n",
       "       [1.5012136 , 0.40861064]], dtype=float32)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(params_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1001)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- grid of plots for these initial 3\n",
    "- hline for optimal solution\n",
    "- ipywidgets\n",
    "- move bulk to plot_helpers.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_array = np.array([0, *iters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iters_array, loss_history)\n",
    "plt.title(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iters_array, g_history)\n",
    "plt.axhline(EPSILON, color='r', linestyle='--')\n",
    "plt.title(r\"$g(x,y)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iters_array, multiplier_history)\n",
    "plt.title(r\"$\\lambda$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fancier visualizations\n",
    "- Parameters in param space with level curves\n",
    "- f vs g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e95f21d6d24b9c057dc89e3e94c7b1d285f49ff39a3b96b3de58d480bdf45a1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('coop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
